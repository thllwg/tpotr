[
["index.html", "TPOT and the Automated Statistician Chapter 1 Introduction", " TPOT and the Automated Statistician Thorben Hellweg Christopher Olbrich Christian Werner 2019-07-03 Chapter 1 Introduction Machine learning has gained in importance over the last years. While machine learning shows promising results in many problem domains, the complexity of machine learning is often an obstacle for its application. This development has given rise to the research field automated machine learning (AutoML) that investigates the problem of “automatically (without human input) producing test set predictions for a new dataset within a fixed computational budget” (Feurer et al. 2015). Characteristics of machine learning like the non-existence of a machine-learning method that performs best on all data sets and the need for hyperparameter optimization has lead to the development of a diverse set of AutoML methods and systems. The book Automatic Machine Learning (Hutter, Kotthoff, and Vanschoren 2018) provides an excellent overview of selected AutoML methods and systems. While the AutoML methods and systems cover a lot of use cases, their different user interfaces and used programing languages contradict the idea of AutoML, which should be easy to use for different addressees. The availability of the AutoML methods and systems in a uniform programmin language like R could help to facilitate their application and comparison. In the described context this reports goal is to investigate the two AutoML methods TPOT and the Automatic Statistician and to document their implementation in R. TPOTs goal is to automate all steps of machine learning after preprocessing was done by generating and optimizing machine learning pipelines (Olson and Moore 2018). The Automatic Statistician wants to automate all steps of machine learning with the special focus of making the predictions available in a human-readable report including graphs and statistics (Steinruecken et al. 2019). To reach this goal both AutoML systems are introduced separately in chapter 2 (TPOT) and chapter 3 (The Automatic Statistician). For each system the core theoretical concept is described, followed by an description of the implementation in R. In the conclusion of this report, a final evaluation of the individual methods and implementations in the context of AutoML’s objectives is made. References "],
["tree-based-pipeline-optimization-with-tpot.html", "Chapter 2 Tree-Based Pipeline Optimization With TPOT 2.1 Tree-Based Pipeline Optimization Tool 2.2 From Theory to Practise: Implementing a Tree-Based Pipeline Optimization Tool", " Chapter 2 Tree-Based Pipeline Optimization With TPOT 2.1 Tree-Based Pipeline Optimization Tool To achieve the goal of automating machine learning the Tree-Based Pipeline Optimization Tool (TPOT) uses genetic programming (GP) for the representation and optimization of machine learning pipelines (Olson and Moore 2018). GP is an evolutionary computation technique that is used to seek models with maximum fit. The special characteristic of GP is the usage of a tree-structure as representation. This characteristic allows for recombination by the exchange of subtrees and mutation by random changes in the tree-structure (Eiben, Smith, and others 2003). TPOT represents machine learning pipelines by treating the machine learning pipeline operators as GP primitives and the data set as GP terminals. This allows arbitrary and flexible pipeline structures. TPOT organizes the machine learning operators into the following categories: Feature preprocessing operators, feature selection operators and supervised classification operators. Fig.1 shows an exemplary pipeline. The data set is manipulated in a successive manner by each operator and the resulting data set is used for Logistic Regression algorithm to solve a classification task (Olson and Moore 2018). Figure 2.1: Exemplary Tree-based pipeline (Olson and Moore 2018) The optimization of the pipeline is done by using a standard genetic programming algorithm. Classification accuracy is used as the pipelines fitness. The GP algorithm generates 100 random pipelines and selects the top 20 pipelines according to the NSGA-II selection scheme nsgaII. During this phase pipelines are selected to maximize classification accuracy and at the same time minimize the number of pipeline operators. Each of the top 20 pipelines produces 5 offspring’s. 5% of the offspring is affected by a crossover using one-point crossover. 90% of the unaffected offspring is randomly changed by a point, insert, or shrink mutation in the tree-structure respectively the machine learning pipeline. After 100 generations the pipeline with the highest classification accuracy is selected from the pareto-front (Olson and Moore 2018). The authors implemented their approach in the python package TPOT.1 The package makes use of the two python packages scikit-learn2 and DEAP.3 While scikit-learn is used for the interfacing of machine-learning operators, DEAP is used for the optimization with GP. The functionality of the python package TPOT goes beyond what is published by the TPOT authors (Olson and Moore 2018) in the AutoML-book (Hutter, Kotthoff, and Vanschoren 2018), because the package allows to manipulate the parameters of the used genetic programming algorithm and is constantly expanded. As an example the publication by (Olson and Moore 2018) only deals with classification, while the python package TPOT also offers regression. 2.2 From Theory to Practise: Implementing a Tree-Based Pipeline Optimization Tool 2.2.1 An R-Wrapper for TPOT 2.2.1.1 Approach Two options for implementing TPOT in R were considered at the beginning. The first is an independent implementation of the underlying theory in R, which - possibly inspired by the Python reference implementation - renounce external dependencies to the extent that the essential algorithms and functions are executed in R. In addition to the obvious advantage that this approach spares users the need to install additional dependencies (programming languages, runtime environments, etc.), there are further benefits: language-dependent advantages of R over Python can be better exploited, maintenance and troubleshooting are simplified. R packages required for implementing TPOT functions are - at least partially - available for both pipeline operators and genetic programming (ecr). However, this also touches on one of the disadvantages of this procedure: Although a range of machine learning packages are available for R (caret, mlr), some of the libraries TPOT uses are not provided as R packages, making it difficult to recreate the reference implementation true to the original. The biggest disadvantage of a stand-alone implementation of TPOT, however, is the resulting separation from the actively developed reference implementation. Since the authors of this seminar paper do not plan to actively maintain the project, the two implementations would have to drift apart immediately with regard to their functional scope. These considerations lead to the second implementation option for TPOT in R, namely wrapping the existing Python implementation in R. The advantages are obvious: With the reticulate package, Python applications can be conveniently wrapped in R. Assuming API stability, the R implementation will then benefit from the continued development of TPOT in Python, which should significantly reduce the maintenance effort in R. Ultimately, this approach allows the functions and results of the Python implementation to be reproduced in a way that would not have been possible with an R-based custom development. Users of the Python variant should be able to determine little to no differences in the results of the application. Thus, it was decided to wrap the existing python package TPOT in a new R package tpotr. The realization is described in the following. 2.2.1.2 Requirements &amp; Implementation Due to the chosen procedure, Python must be supported on the target system. In TPOTs installation documentation, the author recommends the use of the Enterprise Data Science Platform Anaconda.4 Consequently, an installed Anaconda environment is required on the target system of the tpotr-package. The package contains five R files that represent the logic. Following classical conventions, functions required to load the package can be found in the R/zzz.R file. Other functions that are used to install Python libraries can be found in R/requirements.R and R/installation.R. If a user installs tpotr, the function install_tpot() is executed in the packages .onload(). This checks whether Anaconda is installed on the target system and then executes the installation procedure for TPOT using the reticulate packet, if this has not yet been installed. This procedure is repeated each time the package is loaded. For wrapping TPOT, constructor scripts were first created in Python, which can be found at /inst/python/pipeline_generator.py. This script is used by the R functions in R/TPOT.R to create and execute corresponding TPOT objects in Python. #### Integration with mlr The R package mlr is one of the more prominent packages for the execution of machine learning tasks in R (Bischl et al. 2016). Essentially, the same steps are performed for any data sets: Creating an ML-Task, determining which Learner (i.e. which ML-Algorithm) should be applied to the problem and finally training the Learner on the data. While mlr comes with numerous Learners, the user of the package must decide which Learner is to be applied best to the problem at hand. Obviously the integration of TPOT as an mlr-learner can simplify the machine learning process. In accordance with the AutoML concept, this makes the use of machine learning methods more accessible. The provision of an external learner in mlr is described in detail in their documentation5, the corresponding implementation in tpotr is found in the file R/mlr.R. The following code excerpt shows the simple use of textit{tpotr} in mlr using the example of the iris classification task: task = makeClassifTask(data = iris, target = &quot;Species&quot;, id = &quot;iris&quot;) learner = makeLearner(cl = &quot;classif.tpot&quot;, population_size = 10, generations = 3, verbosity = 2) model = train(learner, task) predict(obj = model, newdata = testdata) Due to the structure of mlr, however, the use of tpotr in mlr is not problem-free. In mlr, the prediction type is defined when the learner is created. With a classification problem, the standard predict.type = 'response' returns the predicted classes, while predict.type='prob' returns the probability of the individual classes. Now - if a learner is selected manually - it is clear in advance whether this learner is able to return the probability for classes. However, if TPOT fits a Machine Learning Pipeline, the fitted model may not support this prediction type. Since TPOT itself does not offer the possibility to prevent such pipelines, the user of the package must make use of adequate countermeasures here. For instance, you can train the model in a loop until a pipeline is fitted, that supports the predict.type = 'prob'. task = makeClassifTask(data = iris, target = &quot;Species&quot;, id = &quot;iris&quot;) learner = makeLearner(cl = &quot;classif.tpot&quot;, population_size = 10, generations = 3, verbosity = 2) model = NULL; pred = NULL; # predity.type = &quot;prob&quot; of learner is not supported by every possible # tpot pipeline to ensure that tpot returns a pipeline # that supports the &quot;prob&quot; property, iterate over the training # until such pipeline is found. while(TRUE){ result = try({ model = train(learner, task) pred = predict(obj = model, newdata = testdata) }, silent = TRUE) if (!inherits(result, &quot;try-error&quot;)){ break } } 2.2.1.3 Testing &amp; Documentation The functionality of tpotr is ensured on several levels. By wrapping the Python implementation, the functional tests of the reference implementation take effect first. In the R package the wrapping as well as the integration with mlr is tested by means of the “testthat” package. Finally, with Travis CI - a free and open source software for continuous integration - the successful deployment on different virtual machines is tested after each change. Specifically, the package will be rolled out on a total of 8 virtual machines in different combinations of operating systems and installed Python versions6. The Travis-CI tests are performed after each code change in the Github repository of the project, the .travis.yml file stored in the R package specifies the test environments. In spite of the extensive tests and the different levels at which tests are carried out, the successful completion of these tests does not lead to the conclusion that there are no errors. TPOT uses a number of dependencies that are maintained by different maintainers. Different update cycles can lead to problems between the individual packages, as documented in the BugTracker of the reference implementation. The documentation of the package is manifold. Code written in R is documented by the roxygen2-package. The reasoning and function of the package, as well as installation instructions and troubleshooting hints can be found on the Github page7 of the project and in the package vignettes. In addition, the package contains a number of examples that demonstrate how to use the package. A detailed description of the underlying theory and additional information about the R package can be found in this report, that is also published via R bookdown as a github page. 2.2.1.4 Benchmarking In the context of machine learning, the question of performance inevitably arises. With Auto-ML this is all the more true since experimenting (cf. TPOT) is a frequent component in finding and training machine learning models. The authors of the reference implementation have described the performance of their application in various publications (e.g. (Olson and Moore 2018)). The authors of this report have considered comparative performance studies, but rejected them for the following reasons: For the benchmarking runs to be comparable, not only the processed data sets would have to be the same, but also the pipeline operators tested within the genetic programming framework. However, the fundamental random influence cannot be eliminated by setting the seed of the random number generator. This is because the R implementation has no interface to pass its seed to the wrapped Python functions. Thus the test runs would only be comparable in the context of statistical surveys, i.e. the x-fold execution of tpotr and TPOT. These executions were not possible during the seminar due to time considerations. However, the necessity of benchmarks can be questioned with a simple consideration: The R code in tpotr only plays a critical role when TPOT is being initialized. All other operations (fitting the pipeline, executing predictions, etc.) are completely executed in Python and only the respective end result is transmitted back to the R-interface. This means that the performance of TPOT is only affected by the costs of the R calls. Although R is a comparatively slow language, it can be assumed that the delay is in the range of a few miliseconds. References "],
["implementing-the-automatic-statistician-in-r.html", "Chapter 3 Implementing the Automatic Statistician in R 3.1 The Automatic Statistician: A philiosphy for automating machine learning 3.2 Examples of Automatic Statisticians 3.3 Model-agnostic methods 3.4 AutoStatR: An Automatic Statistician for the R language 3.5 Challenges of the Automatic Statistician", " Chapter 3 Implementing the Automatic Statistician in R This chapter focuses on the Automatic Statistician project which introduces report generating automats for data science incorporating declarative statistics, automated construction and natural language explanation (Steinruecken et al. 2019). In the following, the key characteristics which make up an Automatic Statistician are highlighted. Then, various implementations of the Automatic Statistician are presented. Among others, this includes an Automatic Statistician for the R language (AutoStatR) that was implemented in the context of this seminar paper. Last, challenges and limitations of the Automatic Statistician are explored with one eye on the experience gained when implementing AutoStatR. 3.1 The Automatic Statistician: A philiosphy for automating machine learning In 2014, a project called “The Automatic Statistician” won the $750,000 Google Focused Research Award (CambridgeUniversity 2014). The project, lead by Zoubin Ghahramani, aims to reduce the dozens of man-hour and high-value expertise that are required to select the best combination of models and parameters by automating the process of data science. More than that, the Automatic Statistician produces predictions and human-readable reports from raw datasets while reducing the necessity of human intervention. It consists of several components as described by (Steinruecken et al. 2019): Basic graphs and statistics: A first overview of the dataset’s features is provided. It can be used to prove that the dataset was loaded correctly. Automated construction of models: A suitable model has to be selected from a fixed or open-ended set of models. This model is then trained on the provided dataset. Explanation of the model: The patterns that have been found are explained to the user. There is a certain degree of interpretation. Report curator: A software component that turns these results into a human-readable report. The content of the report fully depends on the dataset and evaluated models. It should give insights about the data to a larger group of people. Over the years, multiple versions of the Automatic Statistician have been build by different people. Each of them has a slightly different purpose, but all of them incorporate the philosophy of the Automatic Statistician and intend to automate data science (Steinruecken et al. 2019). In the next section, there is an overview about what research has shown regarding the Automatic Statistician so far. 3.2 Examples of Automatic Statisticians Over the time, since the Automatic Statistician was announced, numerous authors have contributed to the project with their individual work. For example, (Lloyd et al. 2014) present an Automatic Statistician for regression which explores an open-ended space of models to produce a natural-language report, that was also mentioned by (Steinruecken et al. 2019). They make use of Gaussian Processes and their strength of modelling high-level properties of functions (e.g. smoothness, trends, periodicity) which can be used directly for the model explanation. (Hwang, Tong, and Choi 2015) proceed similar, but construct natural-language descriptions of time-series data. They also make use of Gaussian Processes. While most papers focus on an Automatic Statistician for regression, there has been done research on classification problems as well. (Mrkšić 2014) makes use of earlier work on regression problems with Gaussian Processes and contributes to the project by implementing a model search procedure for classification problems. Clearly, there has been done a lot of research on the Automatic Statistician already. Various authors focused on different types of problems all incorporating the ability of generating human-readable reports. The focus in research done so far was mainly on Gaussian Processes. They provide direct model explanation while constructing the model (Lloyd et al. 2014). Models which incorporate this properties are called interpretable models (Molnar 2019). Interpretability in this context can be seen as “the degree to which an observer can understand the cause of a decision” (Miller 2019) of a machine learning model. While interpretable model provide an easy way of achieving interpretability, they also suffer in terms of flexibility, as each model yields different types of interpretion and thereby binds the developer to the selected model type (Molnar 2019). Indeed, there is a method which provides more flexibility in terms of model selection, model-agnostic methods. This approach was selected for the Automatic Statistician presented in this paper and it is explained in more detail in the subsequent section. 3.3 Model-agnostic methods Model-agnostic methods challenge the task of interpreting and explaining any machine learning models including those that appear as a black box (Molnar 2019). While some machine learning models already incorporate a certain degree of interpretability (such as decision trees), others do clearly not [Molnar (2019)}. In the latter case, model-agnostic methods can be used to seperate the explanation from the machine learning model. These methods are applied to the already optimized machine learning model and provide insights about its behaviour to the developer. They have the advantage that developers are free to select their machine learning model and do not have to choose a fixed model (e.g. Gaussian Processes) which might not be suitable for a certain use case (Molnar 2019). Model-agnostic methods leverage the strength and diversity of the full range of machine learning models while still providing a degree of explanability. In the next section, this alternative approach is used in an Automatic Statistician for the R language. 3.4 AutoStatR: An Automatic Statistician for the R language While Automatic Statisticians so far have been implemented using interpretable models (specifically Gaussian Processes), we introduce an Automatic Statistician build with model-agnostic methods. This R package can use a great variety of different machine learning models and still provide interpretability to the user by exploiting the strength of model-agnostic methods. It follows the philosophy, includes the key components of an Automatic Statistician as presented by (Steinruecken et al. 2019) and can be applied to solve classification problems. Further, it summarizes a data set, re-uses the R-version of TPOT (tpotr) to build a machine learning model, explains this model through model-agnostic methods and outputs an HTML report. In the following subsections the implementation of the four core components of an Automatic Statistician in AutoStatR is described. 3.4.0.1 Data set overview The first component provides an overview of the data set that is loaded into the Automatic Statistician. For this purpose, the package summarytools8 is used. It provides methods for a quick and simple overview of all the features and feature values in the data set and visualizes them in a table format. The following information are provided: The number of the feature indicating the order in which it appears in the dataset The name of the feature and its class An insight into the feature’s values. The frequency, proportions or number of distinct values A histogram or barplot of the feature’s values The number and proportion of valid and missing values in the feature. 3.4.0.2 Machine learning model construction The second component deals with the search and evaluation of machine learning models. Other than proposed by (Steinruecken et al. 2019) who make use of Gaussian Processes, any machine learning model might be used in AutoStatR. We want to use a model that fits the input data set best and not restrict the range of selectable models. This is possible because we split the model search and the model interpretation as described in the previous chapter as model-agnostic methods. Theoretically, AutoStatR can select any interpretable as well as black-box model. Because AutoStatR uses the R implementation of TPOT tpotr, which was introduced previously, it is limited to the models included in the TPOT package. Although the approach of model-agnostic explanation is significantly different from the interpretable Gaussian Processes approach, TPOT fulfills three of the four key ingredients of model search and evaluation introduced by (Steinruecken et al. 2019): First, the open ended language of models is provided by TPOT, because it uses various machine learning operators and models and can construct arbitrary pipelines from them to represent different real world phenomena. Second, TPOT provides a search procedure using genetic programming to explore the language of models. Third, TPOT provides a principled method of evaluating models through genetic programming, which trades of model complexity and fit to data. This is ensured by the NSGA-II selection schema in TPOT (see chapter 2.1). The fourth ingredient “automatic explanation” is not provided by TPOT itself, but by the model-agnostic explanation approach. 3.4.0.3 Model interpretation After gaining an overview over the data set and constructing a machine learning model, the third component of the AutoStatR deals with the model explanation and interpretation. Any Automatic Statistician should be able to make the assumptions of the model explicit to the user in an accurate and intelligible way (Steinruecken et al. 2019). Since the machine learning pipelines generated by TPOT are generally black-box machine learning models, the assumptions of the model can only be made explicit through the usage of model-agnostic methods. The book Interpretable Machine Learning (Molnar 2019) provides an overview of such methods with respective implementations in R. From the model-agnostic methods described in the book, three where found to be suitable for usage in a report. Feature importance is selected as the first method for model interpretation, because it offers a conceptually easy to understand interpretation and a compressed global insight into the model. As the first method being displayed in the AutoStatR report it offers a good entry point for the user. Feature importance is measured as the increase in the prediction error of the model after the feature was permuted. Therefore, feature importance can also be interpreted as the increase in the model error, when a feature is destroyed (Molnar 2019). As the second method accumulated local affects (ALE) are selected, because they describe how an individual feature influences the prediction of the model on average. Thus, ALE offer a more detailed insight into the model than feature importance. Compared to similar methods like partial dependency plots, ALE offer the advantage of providing an unbiased influence if features are correlated (Molnar 2019). In AutoStatR, the two most important features are selected based on the feature importance analysis and subsequently ALE are applied for these features. Lime is selected as the third method. It also provides the most detailed insight into the model, because it looks at individual observations and can therefore be categorized as an interpretable local surrogate model (Molnar 2019). Lime helps to explain the decision of the model for individual observations. Consequently, Lime is used in AutoStatR to explain the decisions of the model for the provided test data. 3.4.0.4 Report curator The last component of AutoStatR is the report curator which wraps the results from the previously explained components in an HTML report. It contains the data set overview, the best fitting model and model explanation. It is based on graphs, tables and natural language descriptions which aims to enhance the comprehensibility for non-experts. Technically, the report curator is located in the file /inst/rmarkdown/report.rmd and is called from the method autostatr() in /R/main.R, when the user starts AutoStatR. The user provides the data set for training, the input data for which the AutoStatR should make predictions, and the target variable. Subsequently, the report is generated automatically. One challenge of the report curator is to generate natural-language descriptions, especially when taking into consideration that the provided data set can have an arbitrary size and the target column can have an arbitrary amount of levels. How this is handled by the report curator is demonstrated exemplary for the description of the first ALE plot in the following. \\ for(i in 1:length(levels)){ level &lt;- levels[i] temp.result2 &lt;- subset(temp.result, .class == level) ale.desc[i] &lt;- paste(&quot;The class &quot;, &quot;&lt;b&gt;&quot;, level, &quot;&lt;/b&gt;&quot;, &quot; is based on the feature &quot;, &quot;&lt;b&gt;&quot;, first.feature, &quot;&lt;/b&gt;&quot;, &quot; most likely for values between &quot;, temp.result[[1,&quot;min&quot;]], &quot; and &quot;, paste0(temp.result[[1,&quot;max&quot;]], &quot;. &quot;)) } Listing 3 shows how the dynamic description is generated. The variable temp.result contains the result of the ALE for the most important feature. More precisely temp.result contains all target class levels that are most likely within a given range of the most important feature. The for loop iterates over these levels and generates a respective natural-language description for every level. &lt;p&gt;For the given classification problem the most important feature according to the feature importance analysis is &lt;b&gt;`r paste(imp.features[1], sep = &quot;, &quot;)`&lt;/b&gt;. The ALE-plot provides an analysis how this feature influences the target &lt;b&gt;`r target`&lt;/b&gt; with its `r length(levels)` classes. `r paste(ale.desc, collapse=&quot;&quot;)`&lt;/p&gt; This generated description, which is saved in ale.desc is then used among other variables in the HTML source code of the report as it can be seen in Listing 4. 3.5 Challenges of the Automatic Statistician Due to conceptually ambitious design of Automatic Statisticians, the implementation of AutoStatR was faced by a variety of challenges, of which some remain open for future research. (Steinruecken et al. 2019) already list several design challenges that go along with the implementation of an Automatic Statistician: User interaction: The user should be able to interact with the system and influence the choices it makes. The system than engages a dialogue with the user to explain the results that were found. Missing and messy data: Some machine learning models struggle with missing data values and other defects on the data set. Therefore, automatic data pre-processing is an important requirement. Resource allocation: Resource constraints such as limited computer power or limited time should be handled by the Automatic Statistician. While TPOT incorporates mechanisms for resource allocation such as processing time restriction and also pre-processes the data in it’s pipelines up to a certain degree (e.g. feature selection), user interaction is still outstanding for AutoStatR. This is because an adequate user interaction requires technology that can provide a user experience such as a web-technology-based solution. Since AutoStatR is an R package, user interaction might be build on top of it. Apart from that, two main challenges remain that were observed during the development and testing of AutoStatR which we want to highlight in the following. 3.5.0.1 Data input Data sets that are given to an Automatic Statistician can be of arbitrary size and quality. First of all, they can range from a few features and observations up to thousands of features and million of rows. Since the output of an Automatic Statistician is a report, this report suffers if data size gets to large because the amount of visualized content gets very complex. The question is whether the report is still a suitable output format to deal with massive data sets. Second, data pre-processing sometimes requires a large amount of manual work which makes up to 70% of all the efforts done in data science projects. The problem is that missing or dirty data in data sets can be so diverse, ranging from missing values over wrong values to miss-spelled column names, that automatic pre-processing can be a project on its own. Of course, missing data can be re-generated through automatic data imputation, but bad data quality is so much more than only missing values and it should be possible for the user to also intervene manually. 3.5.0.2 Trade-off between model fit and interpretability The second point that we want to mention here relates to the principle of model-agnostic methods that was used in AutoStatR. We already mentioned that model-agnostic methods are significantly different from using interpretable machine learning models. Clearly, there is a tradeoff to be made. While interpretable models best explain the inner dependencies of the model but restrict the repertoire of models to select from, the model-agnostic approach might use any machine learning model, but yields less sophisticated explanations if the model gets to complex (Molnar 2019). References "],
["conclusion.html", "Chapter 4 Conclusion", " Chapter 4 Conclusion This report documented and evaluated how two AutoML systems are maid accessible in the programming language R. With tpotr and its interface to mlr there exists now an easy way of using tree-based pipeline optimization in R. Users with basic understanding of R can generate good fitting machine learning pipelines without the need for manual machine learning operator selection and optimization. The implementation of tpotr as an interface to the Python package TPOT provides the advantage of an upward compatibility to improvements in future releases and an active maintenance of the code base. The Automatic Statistician as the second AutoML system brings up the idea of interpretability, since its aim is to generate human readable reports. This can be realized by the usage of an interpretable models like Gaussian Processes. In this report a different approach was evaluated, which separates the model construction from its interpretation. This approach allows for the usage of other well-performing AutoML systems for the model construction part. AutoStatR implements this approach by using tpotr for the model construction and selected model-agnostic methods like ALE for model interpretation. Considering the objectives of AutoML described in the introduction we made several observations during the development phase. The tpotr package clearly reduces the hurdle of machine learning for users that have at least some experience with R. Moreover tpotr provides the opportunity to reduce time spend in machine learning by providing a more experienced user with an insight about a useful pipeline for a given problem. While using tpotr can also have drawbacks, a bigger challenge of AutoML occurs to be its interpretability and applicability for very inexperienced users. AutoStatR provides a possible starting point for these challenges as it works well on selected data sets (which can be found in the example folder of the project), but more work and research needs to be done on challenges like interpretability and handling an arbitrary data input. To contribute to the field of AutoML and especially facilitate its applicability in R, we plan to make tpotr, AutoStatR and a corresponding documentation available as an open source project on GitHub[https://github.com/thllwg/tpotr][https://github.com/thllwg/AutoStatR]. "],
["references.html", "References", " References "]
]
